{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 리뷰 데이터 분류\n",
    "\n",
    "과정은 아래와 같다.\n",
    "\n",
    "1. 데이터는 제공\n",
    "\n",
    "2. 형태소 분석(mecab ver)\n",
    "\n",
    "    2-1. train set에 있는 단어로 사용자 사전 구축\n",
    "    \n",
    "    2-2. 형태소 분석을 한 데이터를 이용(형태소 분석 mecab이 vmware상에 존재해서 속도 측면 때문에 이렇게 진행)\n",
    "    \n",
    "\n",
    "3. 모델링\n",
    "\n",
    "4. 훈련\n",
    "\n",
    "5. 평가\n",
    "\n",
    "6. 번외편(정확성이 아닌 다른 방법으로 테스트 해본 것)\n",
    "\n",
    "    6-1. 머신러닝\n",
    "    \n",
    "    6-2. 카카오 형태소(khaiii)를 이용한 분석(개인 블로그에 설치 및 리뷰 올려놓았다. 해당 편 커널에 주소가 있다.)\n",
    "    \n",
    "    \n",
    "<img src='./assets/0.PNG'>\n",
    "\n",
    "먼저 위와 같이 사용자 사전을 구축한다.\n",
    "\n",
    "<img src='./assets/1.PNG'>\n",
    "<img src='./assets/1-2.PNG'>\n",
    "<img src='./assets/1-3.PNG'>\n",
    "그리고 위 처럼 형태소 분석을 진행하는 파이썬 프로그램을 구현\n",
    "\n",
    "<img src='./assets/2.PNG'>\n",
    "\n",
    "완료.\n",
    "\n",
    "<img src='./assets/3.PNG'>\n",
    "\n",
    "형태소 분석이 완료된 데이터\n",
    "\n",
    "\n",
    "이제 위와 같이 완료된 데이터로 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leesoojin\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, json, glob, sys, numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import keras.backend.tensorflow_backend as K\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Embedding, LSTM, Flatten, Dropout, Input, Conv1D, MaxPooling1D, Activation, GlobalMaxPooling1D, GlobalMaxPool1D\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "헤더가 없으니 헤더를 None으로 해서 가지고 온다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data_after_tokenizer/ratings_train_after_prepro.txt', header=None)\n",
    "data_test = pd.read_csv('./data_after_tokenizer/ratings_test_after_prepro.txt', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train에 15만개, test에 5만개 데이터 셋 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150000\n",
      "(150000, 3)\n",
      "(50000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(len(data.iloc[:, 0]))\n",
    "print(data.shape)\n",
    "print(data_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "뭐 굳이 섞어줄 필요는 없지만 그냥 섞어준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0\n",
      "1    1\n",
      "2    0\n",
      "3    1\n",
      "4    1\n",
      "5    1\n",
      "6    1\n",
      "7    0\n",
      "8    0\n",
      "9    1\n",
      "Name: 2, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = data.sample(frac=1).reset_index(drop=True)\n",
    "print(df.iloc[0:10,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또한, 평가 데이터 중 평가 글이 없는 데이터가 있으므로 제거."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(how='any')\n",
    "data_test = data_test.dropna(how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train, test로 나눈다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.iloc[:, 1].values\n",
    "y_train = df.iloc[:, 2].values\n",
    "X_test = data_test.iloc[:, 1].values\n",
    "y_test = data_test.iloc[:, 2].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['잔잔 흐름 속 피식피식 ' '아주 재밌 고요 저 만화 봐요 ' '시시 전개 시시 결말 촌 전 영화 '\n",
      " '영화 최고 한 번 같 다 다른 세 네 번 최고 배우 ' '쉴새없이 몰아치 는 액션 눈 즐겁 다 년 전 영화 라 믿 어려울 정도 ']\n",
      "['굳 크 ' '뭐 이 평점 .. .. 나쁘 않 지만 10점 더더욱 잖아 '\n",
      " '지루 지 않 은데 완전 막장 돈 주 고 보 .. .. '\n",
      " '었 어도 별 개 줬 을 텐데 .. 왜 나와서 심기 불편 게 하 죠 ? ? ' '음악 주가 된 최고 음악 영화 ']\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:5])\n",
    "print(X_test[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**원래 본인이 하던 스타일은 상위 10%~20% 단어만 사용하는데, 네티즌들이 사용하는 단어는 너무 다양해서 최대한 많은 단어를 사용**\n",
    "\n",
    "그리고 최대 길이가 37이었나? 그래서 길이는 적당히 35로 지정.\n",
    "\n",
    "그리고 Tokenizer를 통해 단어를 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42251\n"
     ]
    }
   ],
   "source": [
    "max_word = 35000\n",
    "max_len = 35\n",
    "\n",
    "tok = Tokenizer(num_words = max_word)\n",
    "tok.fit_on_texts(X_train)\n",
    "print(len(tok.word_index))\n",
    "#print(tok.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트 -> 숫자 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[278, 1095, 160, 9804]\n"
     ]
    }
   ],
   "source": [
    "sequences = tok.texts_to_sequences(X_train)\n",
    "print(len(sequences[0]))\n",
    "print(sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "시퀀스 메트릭스로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148915\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0  278 1095  160 9804]\n"
     ]
    }
   ],
   "source": [
    "sequences_matrix = sequence.pad_sequences(sequences, maxlen=max_len)\n",
    "print(len(sequences_matrix))\n",
    "print(sequences_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_test = tok.texts_to_sequences(X_test)\n",
    "sequences_test_matrix = sequence.pad_sequences(sequences_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with K.tf_ops.device('device:GPU:0'):\n",
    "    model = Sequential()\n",
    "    #max_words를 50차원에. 즉 20000개의 단어를 50차원에다가. 문장의 길이는 max_len\n",
    "    model.add(Embedding(max_word, 50, input_length=max_len))\n",
    "    model.add(LSTM(64))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    model_dir = './model'\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.mkdir(model_dir)\n",
    "    model_path = model_dir + \"/review_lstm_soojin.model\"\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(filepath=model_path, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 35, 50)            1750000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                29440     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,796,337\n",
      "Trainable params: 1,796,337\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 훈련 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 119132 samples, validate on 29783 samples\n",
      "Epoch 1/15\n",
      "119132/119132 [==============================] - 61s 509us/step - loss: 0.3904 - acc: 0.8225 - val_loss: 0.3387 - val_acc: 0.8537\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.33870, saving model to ./model/review_lstm_soojin.model\n",
      "Epoch 2/15\n",
      "119132/119132 [==============================] - 58s 489us/step - loss: 0.2915 - acc: 0.8775 - val_loss: 0.3407 - val_acc: 0.8548\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.33870\n",
      "Epoch 3/15\n",
      "119132/119132 [==============================] - 58s 491us/step - loss: 0.2368 - acc: 0.9021 - val_loss: 0.3784 - val_acc: 0.8535\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.33870\n",
      "Epoch 4/15\n",
      "119132/119132 [==============================] - 58s 489us/step - loss: 0.1957 - acc: 0.9209 - val_loss: 0.3961 - val_acc: 0.8499\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.33870\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(sequences_matrix, y_train, batch_size=128, epochs=15, validation_split=0.2, callbacks=[checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 정확도 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49631/49631 [==============================] - 36s 716us/step\n",
      "정확도 : 0.8460\n"
     ]
    }
   ],
   "source": [
    "print(\"정확도 : %.4f\" % (model.evaluate(sequences_test_matrix, y_test)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 번외편\n",
    "\n",
    "** 사이킷런 및 카카오 형태소 분석기로 했을 때의 성능 비교 **\n",
    "\n",
    "** 카카오 형태소 분석기는 본인 블로그에 설치 방법 및 리뷰를 등록해놓았다. 참고**\n",
    "\n",
    "**설치 및 간단 리뷰**\n",
    "https://lsjsj92.tistory.com/408\n",
    "\n",
    "**모델 비교 및 성능 평가**\n",
    "https://lsjsj92.tistory.com/410\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사이킷런\n",
    "\n",
    "TfidfVectorizer와 logisticregression 이용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최상의 교차 검증 점수 : 0.82\n"
     ]
    }
   ],
   "source": [
    "pipe = make_pipeline(TfidfVectorizer(min_df=5), LogisticRegression())\n",
    "param_grid = {\n",
    "    'logisticregression__C' : [0.001, 0.01, 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"최상의 교차 검증 점수 : %.2f\" %(grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 테스트 점수 : 0.82\n"
     ]
    }
   ],
   "source": [
    "print(\" 테스트 점수 : %.2f\" %(grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = grid.best_estimator_.named_steps[\"tfidfvectorizer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argsort(vector.idf_)[::-1]\n",
    "features = vector.get_feature_names()\n",
    "top_n = 2\n",
    "top_features = [features[i] for i in indices[:top_n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf가 가장 높은 특성 100 개 : \n",
      "  ['승부' '승리' '승려' '습니당' '습니다' '습격' '슬플' '슬프' '슬펐' '캐리' '슬퍼요' '슬퍼' '캐릭터'\n",
      " '스피드' '스포일러' '스페인' '시간대' '스릴러' '친절' '친일파' '씨발' '씁쓸' '쓴다' '쓰레기통' '충무로'\n",
      " '충분' '충분히' '씨엔블루' '쓰레기' '쓰래기' '쏘우' '쏘리' '썸머' '썰전' '썰렁' '취미' '쓰레' '아가'\n",
      " '아기자기' '아까' '아래' '아라' '아들' '아드레날린' '아동' '아담' '아니하' '아니야' '아니' '아놔' '아나'\n",
      " '아깝' '출연료' '아까웠' '출연진' '아까워' '아까운' '써요' '신데렐라' '쌍둥이' '싸이코' '실질' '실은' '실수'\n",
      " '실사' '실망' '치즈' '치킨' '실패' '치히로' '친구' '신하균' '친다' '신파' '신선' '신부' '신발' '칙칙'\n",
      " '실패작' '실화' '심각' '싸이' '취한다' '싸움' '싸우' '취향' '싸구려' '싱크로' '싱겁' '심형래' '심했'\n",
      " '심한데' '심하' '심장' '심심풀이' '치밀' '심심' '심리' '취지' '0개']\n"
     ]
    }
   ],
   "source": [
    "X_train_tf = vector.transform(X_train)\n",
    "feature_names = np.array(vector.get_feature_names())\n",
    "max_value = X_train_tf.max(axis=0).toarray().ravel()\n",
    "sorted_by_tfidf = max_value.argsort()\n",
    "print(\"tfidf가 가장 높은 특성 100 개 : \\n \", feature_names[sorted_by_tfidf[-100:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_estimator_.named_steps[\"logisticregression\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 카카오 형태소 분석기 기준\n",
    "\n",
    "성능이 더 떨어짐. 여러가지 이유가 있겠지만 사용자 정의 사전 미구축 및 은어 등의 단어를 인식을 못하는 문제일 가능성이 높다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data_after_tokenizer_kakao/ratings_train_after_prepro.txt', header=None)\n",
    "data_test = pd.read_csv('./data_after_tokenizer_kakao/ratings_test_after_prepro.txt', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    1\n",
      "4    0\n",
      "5    1\n",
      "6    0\n",
      "7    1\n",
      "8    1\n",
      "9    0\n",
      "Name: 2, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = data.sample(frac=1).reset_index(drop=True)\n",
    "print(df.iloc[0:10,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(how='any')\n",
    "data_test = data_test.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.iloc[:, 1].values\n",
    "y_train = df.iloc[:, 2].values\n",
    "X_test = data_test.iloc[:, 1].values\n",
    "y_test = data_test.iloc[:, 2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34388\n"
     ]
    }
   ],
   "source": [
    "max_word = 32000\n",
    "max_len = 35\n",
    "\n",
    "tok = Tokenizer(num_words = max_word)\n",
    "tok.fit_on_texts(X_train)\n",
    "print(len(tok.word_index))\n",
    "#print(tok.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tok.texts_to_sequences(X_train)\n",
    "sequences_matrix = sequence.pad_sequences(sequences, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_test = tok.texts_to_sequences(X_test)\n",
    "sequences_test_matrix = sequence.pad_sequences(sequences_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with K.tf_ops.device('device:GPU:0'):\n",
    "    model = Sequential()\n",
    "    #max_words를 50차원에. 즉 20000개의 단어를 50차원에다가. 문장의 길이는 max_len\n",
    "    model.add(Embedding(max_word, 50, input_length=max_len))\n",
    "    model.add(LSTM(64))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    model_dir = './model'\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.mkdir(model_dir)\n",
    "    model_path = model_dir + \"/review_lstm_soojin.model\"\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(filepath=model_path, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 119996 samples, validate on 29999 samples\n",
      "Epoch 1/15\n",
      "119996/119996 [==============================] - 61s 505us/step - loss: 0.5612 - acc: 0.6961 - val_loss: 0.5305 - val_acc: 0.7175\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53049, saving model to ./model/review_lstm_soojin.model\n",
      "Epoch 2/15\n",
      "119996/119996 [==============================] - 59s 489us/step - loss: 0.4865 - acc: 0.7620 - val_loss: 0.5324 - val_acc: 0.7183\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.53049\n",
      "Epoch 3/15\n",
      "119996/119996 [==============================] - 58s 484us/step - loss: 0.4489 - acc: 0.7785 - val_loss: 0.5504 - val_acc: 0.7172\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.53049\n",
      "Epoch 4/15\n",
      "119996/119996 [==============================] - 58s 486us/step - loss: 0.4270 - acc: 0.7872 - val_loss: 0.5546 - val_acc: 0.7211\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.53049\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(sequences_matrix, y_train, batch_size=128, epochs=15, validation_split=0.2, callbacks=[checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49997/49997 [==============================] - 35s 702us/step\n",
      "정확도 : 0.7188\n"
     ]
    }
   ],
   "source": [
    "print(\"정확도 : %.4f\" % (model.evaluate(sequences_test_matrix, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
